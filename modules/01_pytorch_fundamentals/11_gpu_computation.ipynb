{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU Numerical Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se mencionó anteriormente en el curso, las computaciones numéricas realizadas por gpu superan enormemente a las realizadas por cpu en términos de tiempo de ejecución y eficiencia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ser capaces de acceder a este tipo de computaciones, es necesario hacer uso de:\n",
    "\n",
    "* Nvidia GPU\n",
    "* CUDA y CUDNN\n",
    "* PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer uso de gpus potentes requeridos por algoritmos poderosos, tenemos las siguientes opciones:\n",
    "\n",
    "1. Usar Google Colab (GPU gratis y opciones de suscripción)\n",
    "2. Nvidia GPU local (Requiere set-up con los drivers de Nvidia, además de CUDA y CUDNN)\n",
    "3. Usar cloud computing (Servicio de suscripción como: AWS, GCP o Azure)\n",
    "\n",
    "Para las opciones 2) y 3) es necesario hacer el set-up y configuraciones necesarias con Nvidia drivers, CUDA, CUDNN y PyTorch\n",
    "\n",
    "Referencias:\n",
    "\n",
    "* CUDA: https://developer.nvidia.com/cuda-downloads\n",
    "* CUDNN: https://developer.nvidia.com/cudnn\n",
    "* PyTorch: https://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisar Especificaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jan  2 15:02:33 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4050 ...    On  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   44C    P5              4W /   35W |     433MiB /   6141MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1707      G   /usr/lib/xorg/Xorg                            112MiB |\n",
      "|    0   N/A  N/A      1870      G   /usr/bin/gnome-shell                          111MiB |\n",
      "|    0   N/A  N/A      2922      G   /usr/lib/firefox-esr/firefox-esr              139MiB |\n",
      "|    0   N/A  N/A      3853      G   ...erProcess --variations-seed-version         35MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En mi caso, el gpu es: NVIDIA GeForce RTX 4050"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisar Acceso a GPU con PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para revisar si CUDA está configurado correctamente y es posible utilizarlo para nuestros modelos se corre el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-Up Device Agnostic Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante crear código agnostico al device utilizado (CPU o GPU) debido a que en caso de que se tenga acceso a un GPU con CUDA deberíamos utilizarlo pero en caso contrario nuestro código debe correr en máquinas sin acceso a ello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente línea configura donde se correrá nuestro código. En caso de que detecte CUDA configurado correctamente se selecciona del GPU, de lo contrario se utiliza el CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante mencionar que este método es utilizado con jupyter notebooks, para scripts de python se utiliza otro método\n",
    "\n",
    "Revisar la documentación de CUDA y PyTorch, además de las buenas prácticas recomendadas: https://pytorch.org/docs/stable/notes/cuda.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conteo de GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependiendo el área de trabajo y el proyecto asociado es posible que se tenga a disposición múltiples GPUs con el propósito de correr paralelamente los algoritmos\n",
    "\n",
    "Para revisar a cuantos se tiene acceso se utiliza el siguiente comando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crear Modelos en el GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que queremos hacer uso de nuestro GPU para agilizar las operaciones tensoriales de nuestros modelos, podemos hacerlo de la siguiente forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), device(type='cpu'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tns = torch.tensor([1, 2, 3])\n",
    "tns, tns.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mover Tensores al GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, nuestros tensores serán creados en el CPU\n",
    "\n",
    "Es posible tanto crearlos directamente en el GPU, como moverlos del device utilizado al device deseado\n",
    "\n",
    "Para mover un tensor A de device podemos: \n",
    "\n",
    "1. Hacer uso de `A.to(device=\"cuda\")` \n",
    "\n",
    "\n",
    "2. Especificar previamente `device = \"cuda\" if torch.cuda.is_available() else \"cpu\"` y luego usar `A.to(device)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tns = tns.to(device)\n",
    "gpu_tns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tns.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que solo tenemos acceso a 1 GPU, nos indica que nuestro tensor está en nuestro GPU con índice 0 (recordando que la numeración simpre inicia en 0). En caso de que tuvieramos acceso a múltiples GPUs es posible indicar que cual deseamos crear nuestros modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Los modelos en GPU no pueden transformarse con Numpy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresar Tensores al CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que los tensores en GPU no pueden operar con Numpy, es importante controlar el device se tienen los tensores\n",
    "\n",
    "Para ello, usamos en el tensor A (actualmente en el gpu) `A.cpu()` y lo guardamos en otra variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), device(type='cpu'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_tns = gpu_tns.cpu()\n",
    "cpu_tns, cpu_tns.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez realizado el cambio de device, es posible operar con Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_tns.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debido a que al realizar el cambio de device se hace una copia del tensor original en el nuevo device y éstas no comparten memoria, nuestro tensor original se mantiene en su device correspondiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3], device='cuda:0'), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_tns, gpu_tns.device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
